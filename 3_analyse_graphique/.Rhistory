geom_bar(stat = "identity")
ggplot(result, aes(x = as.factor(reorder(Province, count(Province))), y = count(Province))) +
geom_bar(stat = "identity")
esquisse:::esquisser()
ggplot(data = result) +
aes(x = reorder(Province), fill = Province) +
geom_bar() +
theme_minimal()
ggplot(data = result) +
aes(x = Province, fill = Province) +
geom_bar() +
theme_minimal()
ggThemeAssist:::ggThemeAssistAddin()
ggplot(data = result) +
aes(x = reorder(miRNA, -count()), fill = Province) +
geom_bar() +
theme_minimal()
ggplot(data = result) +
aes(x = reorder(Province, -count()), fill = Province) +
geom_bar() +
theme_minimal()
ggplot(data = result) +
aes(x = reorder(Province, -count), fill = Province) +
geom_bar() +
theme_minimal()
ggplot(data = result) +
aes(x = reorder(Province, -count), fill = Province) +
geom_bar() +
theme_minimal()
ggplot(data = result) +
aes(x = Province, fill = Province) +
geom_bar() +
theme_minimal()
esquisse:::esquisser()
ggedit:::ggeditAddin()
ggedit:::ggeditAddin()
ggplot(data = result) +
aes(x = Province, fill = Province) +
geom_bar() +
theme_minimal()
ggplot(data = result) +
aes(x = commune) +
geom_bar() +
theme_minimal()
ggplot(data = result) +
aes(x = Province) +
geom_bar() +
theme_minimal()
# exemple de graphique barchart avec ggplot2
ggplot(data = result) +
aes(x = Province, fill=Province) +
geom_bar() +
theme_minimal()
library(rvest)
html = read_html(url)
library(rvest)
mon_url = "http://electionslocales.wallonie.be/2012/fr/com/preferred/preferred_CGM54007_3.html"
html = read_html(mon_url)
html
html_nodes(html, "#wrn_background_Id b")
elus_html = html_nodes(html, "#wrn_background_Id b")
html_text(elus_html)
mon_url %>%
read_html() %>%
html_nodes("#wrn_background_Id b") %>%
html_text()
resultat =
mon_url %>%
read_html() %>%
html_nodes("#wrn_background_Id b") %>%
html_text()
library(rvest)
mon_url = "http://electionslocales.wallonie.be/2012/fr/com/preferred/preferred_CGM54007_3.html"
html = read_html(mon_url)
position_table = html_nodes(html, "#wrn_background_Id")
html_table(position_table)
html_table(html)
html_table(html, fill=TRUE)
html_table(html, fill=TRUE)[34]
listes <- read_csv("C:/Users/ettor/Desktop/AJPRO_dataj/1_scraping/data/liens_listes_wallonie_bruxelles.csv")
View(listes)
liens_listes_wallonie_bruxelles <- read_csv("C:/Users/ettor/Desktop/AJPRO_dataj/1_scraping/data/liens_listes_wallonie_bruxelles.csv")
View(liens_listes_wallonie_bruxelles)
liens_listes_wallonie_bruxelles$liens_listes_wallonie_bruxelles
listes = liens_listes_wallonie_bruxelles$liens_listes_wallonie_bruxelles
library(rvest)
library(readr)
#On importe la liste de liens récupérés à l'aide de web scraper chrome
liens_listes_wallonie_bruxelles <- read_csv("C:/Users/ettor/Desktop/AJPRO_dataj/1_scraping/data/liens_listes_wallonie_bruxelles.csv")
View(liens_listes_wallonie_bruxelles)
listes = liens_listes_wallonie_bruxelles$liens_listes_wallonie_bruxelles
tables = vector()
for (i in listes) {
html = i %>%
read_html()
table = html %>%
html_node("#wrn_background_Id") %>%
html_table(header = FALSE,
fill = TRUE,
dec = ",")
#on cree trois colonnes avec le nom de la commune, le nom de la liste et l'URL de la page
table$commune = html %>% html_node(".uppercaseb") %>% html_text()
table$parti = html %>% html_node(".subtitle") %>% html_text()
table$lien = i
#On n'oublie surtout pas de fusionner chaque table avec la liste tables
tables = rbind(tables, table)
}
library(rvest)
library(readr)
#On importe la liste de liens récupérés à l'aide de web scraper chrome
liens_listes_wallonie_bruxelles <- read_csv("C:/Users/ettor/Desktop/AJPRO_dataj/1_scraping/data/liens_listes_wallonie_bruxelles.csv")
View(liens_listes_wallonie_bruxelles)
listes = liens_listes_wallonie_bruxelles$liens_listes_wallonie_bruxelles
tables = vector()
for (i in listes[0:100]) {
html = i %>%
read_html()
table = html %>%
html_node("#wrn_background_Id") %>%
html_table(header = FALSE,
fill = TRUE,
dec = ",")
#on cree trois colonnes avec le nom de la commune, le nom de la liste et l'URL de la page
table$commune = html %>% html_node(".uppercaseb") %>% html_text()
table$parti = html %>% html_node(".subtitle") %>% html_text()
table$lien = i
#On n'oublie surtout pas de fusionner chaque table avec la liste tables
tables = rbind(tables, table)
}
#on nettoye un peu (pourrait se faire dans OpenRefine)
tables = tables %>% drop_na(X2) %>% select(X2:X10, commune:lien)
#On renomme les colonnes (pourrait se faire dans OpenRefine ou dans le csv)
colnames(tables) = c(
"place_liste",
"nom",
"voix",
"pourcent_liste",
"voix_devolues",
"voix_apres_devolution",
"voix_restantes",
"position_elu",
"position_suppleance",
"commune",
"parti",
"lien"
)
View(table)
View(tables)
View(tables)
library(rvest)
library(readr)
#On importe la liste de liens récupérés à l'aide de web scraper chrome
liens_listes_wallonie_bruxelles <-
read_csv(
"C:/Users/ettor/Desktop/AJPRO_dataj/1_scraping/data/liens_listes_wallonie_bruxelles.csv"
)
#on jette un oeil sur le dataframe
View(liens_listes_wallonie_bruxelles)
##############################################################################
listes = liens_listes_wallonie_bruxelles$liens
liste[1]
listes[1]
listes[1] %>%
read_html()
listes[1] %>%
read_html() %>%
html_nodes("#wrn_background_Id")
listes[1] %>%
read_html() %>%
html_nodes("#wrn_background_Id") %>%
html_table()
listes[1] %>%
read_html() %>%
html_nodes("#wrn_background_Id") %>%
html_table(header = FALSE)
listes[1] %>%
read_html() %>%
html_nodes("#wrn_background_Id") %>%
html_table(header = FALSE, fill=TRUE)
listes[1] %>%
read_html() %>%
html_nodes("#wrn_background_Id") %>%
html_table(header = FALSE, fill=TRUE, dec=",")
listes[2] %>%
read_html() %>%
html_nodes("#wrn_background_Id") %>%
html_table()
listes[3] %>%
read_html() %>%
html_nodes("#wrn_background_Id") %>%
html_table()
listes[500] %>%
read_html() %>%
html_nodes("#wrn_background_Id") %>%
html_table()
listes[1:100] %>%
read_html() %>%
html_nodes("#wrn_background_Id") %>%
html_table()
for (url in listes[0:10]) {
url %>%
read_html() %>%
html_nodes("#wrn_background_Id") %>%
html_table()
}
for (url in listes[0:10]) {
url %>%
read_html() %>%
html_nodes("#wrn_background_Id") %>%
html_table()
}
for (url in listes[0:10]) {
table =
url %>%
read_html() %>%
html_nodes("#wrn_background_Id") %>%
html_table()
}
table
for (url in listes[0:10]) {
table =
url %>%
read_html() %>%
html_nodes("#wrn_background_Id") %>%
html_table()
tables = rbind(tables, table)
}
tables = vector()
tables = vector()
for (url in listes[0:10]) {
table =
url %>%
read_html() %>%
html_nodes("#wrn_background_Id") %>%
html_table()
tables = rbind(tables, table)
}
tables
View(tables)
tables = vector()
for (url in listes[0:10]) {
table =
url %>%
read_html() %>%
html_nodes("#wrn_background_Id") %>%
html_table()
tables = rbind(table, tables)
}
tables
for (url in listes[0:10]) {
table =
url %>%
read_html() %>%
html_nodes("#wrn_background_Id") %>%
html_table()
}
table
for (url in listes[0:10]) {
table =
url %>%
read_html() %>%
html_nodes("#wrn_background_Id") %>%
html_table()
}
table
tables_jointes = vector()
tables_jointes = vector()
for (url in listes[0:10]) {
table =
url %>%
read_html() %>%
html_nodes("#wrn_background_Id") %>%
html_table()
tables_jointes = rbind(tables_jointes, table)
}
library(rvest)
library(readr)
#On importe la liste de liens récupérés à l'aide de web scraper chrome
liens_listes_wallonie_bruxelles <-
read_csv(
"C:/Users/ettor/Desktop/AJPRO_dataj/1_scraping/data/liens_listes_wallonie_bruxelles.csv"
)
#on jette un oeil sur le dataframe
View(liens_listes_wallonie_bruxelles)
##############################################################################
listes = liens_listes_wallonie_bruxelles$liens
###############################################################################
tables_jointes = vector()
for (url in listes[0:10]) {
table =
url %>%
read_html() %>%
html_nodes("#wrn_background_Id") %>%
html_table()
tables_jointes = rbind(tables_jointes, table)
}
tables_jointes
tables_jointes[0]
tables_jointes = vector()
for (url in listes[0:10]) {
table =
url %>%
read_html() %>%
html_nodes("#wrn_background_Id") %>%
html_table(fill=TRUE)
tables_jointes = rbind(tables_jointes, table)
}
tables_jointes[0]
tables_jointes
tables_jointes = vector()
for (url in listes[0:10]) {
html =
url %>% read_html()
table =
html %>%
html_nodes("#wrn_background_Id") %>%
html_table(fill=TRUE)
tables_jointes = rbind(tables_jointes, table)
}
tables_jointes[0]
tables_jointes
listes[0:10]
tables_jointes = vector()
for (url in listes[0:10]) {
html = url %>%
read_html()
table = html %>%
html_node("#wrn_background_Id") %>%
html_table(header = FALSE,
fill = TRUE,
dec = ",")
#On n'oublie surtout pas de fusionner chaque table avec la liste tables
tables_jointes = rbind(tables_jointes, table)
}
tables_jointes
View(tables_jointes)
tables_jointes = vector()
for (url in listes[0:10]) {
html =
url %>% read_html()
table =
html %>%
html_node("#wrn_background_Id") %>%
html_table(fill=TRUE)
tables_jointes = rbind(tables_jointes, table)
}
tables_jointes
tables_jointes = vector()
for (url in listes[0:10]) {
html =
url %>% read_html()
table =
html %>%
html_node("#wrn_background_Id") %>%
html_table()
tables_jointes = rbind(tables_jointes, table)
}
tables_jointes
tables_jointes = vector()
for (url in listes[100:120]) {
html =
url %>% read_html()
table =
html %>%
html_node("#wrn_background_Id") %>%
html_table()
tables_jointes = rbind(tables_jointes, table)
}
tables_jointes
View(tables_jointes)
library(rvest)
library(readr)
#On importe la liste de liens récupérés à l'aide de web scraper chrome
liens_listes_wallonie_bruxelles <-
read_csv(
"C:/Users/ettor/Desktop/AJPRO_dataj/1_scraping/data/liens_listes_wallonie_bruxelles.csv"
)
#on jette un oeil sur le dataframe
View(liens_listes_wallonie_bruxelles)
##############################################################################
listes = liens_listes_wallonie_bruxelles$liens
###############################################################################
tables_jointes = vector()
for (url in listes[100:120]) {
html =
url %>% read_html()
table =
html %>%
html_node("#wrn_background_Id") %>%
html_table()
####################################################################
#on cree trois colonnes avec le nom de la commune, le nom de la liste et l'URL de la page
table$commune = html %>% html_node(".uppercaseb") %>% html_text()
table$parti = html %>% html_node(".subtitle") %>% html_text()
table$lien = i
#####################################################################
#On n'oublie surtout pas de fusionner chaque table avec la liste tables
tables_jointes = rbind(tables_jointes, table)
}
tables_jointes
View(tables_jointes)
install.packages(c("callr", "car", "fs", "ggthemes", "sjPlot", "wordcloud"))
View(tables)
maListe = vector("ettore", "jean")
maListe = c("ettore", "jean")
maListe
len(maListe)
length(maListe)
maListe[1]
multiplieParDeux = function(nombre) {
nombre * 2
}
multiplieParDeux(4)
multiplieParDeux("ettore")
multiplieParDeux = function(nom) {
strrep(nom, 6)
}
multiplieParDeux("ettore")
multiplieParDeux = function(nom) {
strrep(nom, 6)
}
multiplieParDeux("ettore ")
multiplieParDeux = function(nom) {
#multiplie par deux
strrep(nom, 6)
}
multiplieParDeux("ettore ")
multiplie <- function(x,y) {
x * y
}
multiplie(4,5)
multiplie <- function(x,y,z) {
x * y - z
}
multiplie(4,5, 10)
library(rvest)
library(dplyr)
library(readr)
library(ggplot2)
#On importe et visualise le fichier csv issu des scraping
data =
read_csv("scrapings_wallonie_bruxelles.csv")
View(data)
#1 On crée une copie du dataframe data sans les trois derniers de chaque liste
#(Chercher une solution plus élégante que trois filter)
data_filtered =
data %>%
group_by(liste) %>%
filter(place_liste != max(place_liste)) %>%
filter(place_liste != max(place_liste)) %>%
filter(place_liste != max(place_liste))
View(data_filtered)
#calculer la position sur la liste du dernier du groupe de tête d'élus par liste(sur base du premier non elu)
#calculer la différence entre cette valeur et la position sur la liste des élus
#on pourrait affiner en calculant la place du "premier non élu non suivi d'un élu", mais KISS
result = data_filtered %>%
group_by(liste) %>%
mutate(last_elu = place_liste[which(is.na(position_elu))[1]] - 1, # trouvé sur SO
diff_avec_last=place_liste-last_elu) %>%
filter(elu=="oui") %>%
#question : doit-on privilégier la différence entre la place-place_elu ou l'autre ?
arrange(desc(place_positionelu), desc(diff_avec_last)) %>%
filter(diff_avec_last > 20)
View(result)
setwd("~/GitHub/AJPRO_dataj/3_analyse_graphique")
library(dplyr)
library(readr)
library(ggplot2)
#On importe et visualise le fichier csv issu des scraping
data =
read_csv("scrapings_wallonie_bruxelles.csv")
View(data)
#1 On crée une copie du dataframe data sans les trois derniers de chaque liste
#(Chercher une solution plus élégante que trois filter)
data_filtered =
data %>%
group_by(liste) %>%
filter(place_liste != max(place_liste)) %>%
filter(place_liste != max(place_liste)) %>%
filter(place_liste != max(place_liste))
View(data_filtered)
#calculer la position sur la liste du dernier du groupe de tête d'élus par liste(sur base du premier non elu)
#calculer la différence entre cette valeur et la position sur la liste des élus
#on pourrait affiner en calculant la place du "premier non élu non suivi d'un élu", mais KISS
result = data_filtered %>%
group_by(liste) %>%
mutate(last_elu = place_liste[which(is.na(position_elu))[1]] - 1, # trouvé sur SO
diff_avec_last=place_liste-last_elu) %>%
filter(elu=="oui") %>%
#question : doit-on privilégier la différence entre la place-place_elu ou l'autre ?
arrange(desc(place_positionelu), desc(diff_avec_last)) %>%
filter(diff_avec_last > 20)
View(result)
write_csv(result, "resultats.csv")
library(dplyr)
library(readr)
library(ggplot2)
#On importe et visualise le fichier csv issu des scraping
data =
read_csv("scrapings_wallonie_bruxelles.csv")
View(data)
#1 On crée une copie du dataframe data sans les trois derniers de chaque liste
#(Chercher une solution plus élégante que trois filter)
data_filtered =
data %>%
group_by(liste) %>%
filter(!(place_liste %in% top_n(3)))
data_filtered =
data %>%
group_by(liste) %>%
filter(!(place_liste %in% top_n(data_filtered$place_liste, 3)))
top_n(data_filtered$place_liste, 3)
